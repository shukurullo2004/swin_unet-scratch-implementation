{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc829fbd-bff1-40b9-a138-e640708a4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3be9e5-26fd-45a9-afbb-ef8a41bab09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile  # Add this import statement\n",
    "# import kagglehub\n",
    "# dschettler8845_brats_2021_task1_path = kagglehub.dataset_download('dschettler8845/brats-2021-task1')\n",
    "\n",
    "# print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "491232c3-38af-4d71-b4ca-0152dff62b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# zip_file = tarfile.open(\"/root/.cache/kagglehub/datasets/dschettler8845/brats-2021-task1/versions/1/BraTS2021_Training_Data.tar\")\n",
    "# zip_file.extractall(\"/data\")\n",
    "# zip_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57cadec-fe66-4d7f-881e-833ff3517f2a",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1c038a-3340-4a8e-8a4a-b1ed86e4b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "\n",
    "class BraTS2021Dataset(Dataset):\n",
    "    \"\"\"BraTS 2021 dataset\"\"\"\n",
    "    def __init__(self, data_dir, patch_size=(128, 128, 128), transform=None, seed=42):\n",
    "        self.data_dir = data_dir\n",
    "        self.patch_size = patch_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        # Get subject folders - look for all directories in the data_dir\n",
    "        print(f\"Looking for data in: {data_dir}\")\n",
    "        subject_dirs = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "        \n",
    "        if not subject_dirs:\n",
    "            # Try looking for .nii.gz files directly\n",
    "            nifti_files = glob.glob(os.path.join(data_dir, \"**\", \"*.nii.gz\"), recursive=True)\n",
    "            if nifti_files:\n",
    "                # Extract unique subject IDs from filenames\n",
    "                subject_ids = set()\n",
    "                for file in nifti_files:\n",
    "                    basename = os.path.basename(file)\n",
    "                    # Extract subject ID (assuming format like \"BraTS2021_00000_t1.nii.gz\")\n",
    "                    subject_id = '_'.join(basename.split('_')[:-1])  # Remove modality suffix\n",
    "                    if '_seg' in basename:  # Handle segmentation files\n",
    "                        subject_id = subject_id.replace('_seg', '')\n",
    "                    subject_ids.add(subject_id)\n",
    "                \n",
    "                # Create a list of subject paths\n",
    "                self.subjects = [os.path.join(data_dir, subject_id) for subject_id in subject_ids]\n",
    "                print(f\"Found {len(self.subjects)} subjects based on .nii.gz files\")\n",
    "            else:\n",
    "                raise ValueError(f\"No subjects found in {data_dir}. Please check the data path and structure.\")\n",
    "        else:\n",
    "            self.subjects = [os.path.join(data_dir, d) for d in subject_dirs]\n",
    "            print(f\"Found {len(self.subjects)} subject directories\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        subject_path = self.subjects[idx]\n",
    "        subject_id = os.path.basename(subject_path)\n",
    "        \n",
    "        # Try to locate the MRI files\n",
    "        def find_file(pattern):\n",
    "            matches = glob.glob(os.path.join(self.data_dir, \"**\", pattern), recursive=True)\n",
    "            if matches:\n",
    "                return matches[0]\n",
    "            return None\n",
    "        \n",
    "        # Try different possible file patterns\n",
    "        t1_path = find_file(f\"{subject_id}*t1.nii.gz\") or find_file(f\"*{subject_id}*t1.nii.gz\")\n",
    "        t1ce_path = find_file(f\"{subject_id}*t1ce.nii.gz\") or find_file(f\"*{subject_id}*t1ce.nii.gz\")\n",
    "        t2_path = find_file(f\"{subject_id}*t2.nii.gz\") or find_file(f\"*{subject_id}*t2.nii.gz\")\n",
    "        flair_path = find_file(f\"{subject_id}*flair.nii.gz\") or find_file(f\"*{subject_id}*flair.nii.gz\")\n",
    "        seg_path = find_file(f\"{subject_id}*seg.nii.gz\") or find_file(f\"*{subject_id}*seg.nii.gz\")\n",
    "        \n",
    "        if not all([t1_path, t1ce_path, t2_path, flair_path, seg_path]):\n",
    "            print(f\"Missing files for subject {subject_id}. Found: {t1_path}, {t1ce_path}, {t2_path}, {flair_path}, {seg_path}\")\n",
    "            # Return a dummy sample if files are missing\n",
    "            # This prevents crashes but you should check your data\n",
    "            image = np.zeros((4, *self.patch_size), dtype=np.float32)\n",
    "            mask = np.zeros((3, *self.patch_size), dtype=np.float32)\n",
    "            return torch.from_numpy(image).float(), torch.from_numpy(mask).float()\n",
    "        \n",
    "        # Load data\n",
    "        t1 = self.load_and_normalize(t1_path)\n",
    "        t1ce = self.load_and_normalize(t1ce_path)\n",
    "        t2 = self.load_and_normalize(t2_path)\n",
    "        flair = self.load_and_normalize(flair_path)\n",
    "        seg = self.load_nifti_volume(seg_path)\n",
    "        \n",
    "        # Stack modalities\n",
    "        image = np.stack([t1, t1ce, t2, flair], axis=0)  # (4, H, W, D)\n",
    "        \n",
    "        # Create segmentation masks for the three tumor sub-regions\n",
    "        mask_et = (seg == 4).astype(np.float32)\n",
    "        mask_wt = ((seg == 1) | (seg == 2) | (seg == 4)).astype(np.float32)\n",
    "        mask_tc = ((seg == 1) | (seg == 4)).astype(np.float32)\n",
    "        \n",
    "        mask = np.stack([mask_et, mask_wt, mask_tc], axis=0)  # (3, H, W, D)\n",
    "        \n",
    "        # Random crop for data augmentation\n",
    "        image, mask = self.random_crop_3d(image, mask)\n",
    "        \n",
    "        # Apply augmentations\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "        \n",
    "        return torch.from_numpy(image).float(), torch.from_numpy(mask).float()\n",
    "    \n",
    "    def load_nifti_volume(self, filepath):\n",
    "        \"\"\"Load a NIfTI volume\"\"\"\n",
    "        try:\n",
    "            nifti = nib.load(filepath)\n",
    "            volume = nifti.get_fdata().astype(np.float32)\n",
    "            return volume\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filepath}: {e}\")\n",
    "            # Return an empty volume as placeholder\n",
    "            return np.zeros((240, 240, 155), dtype=np.float32)\n",
    "    \n",
    "    def load_and_normalize(self, filepath):\n",
    "        \"\"\"Load and normalize a volume\"\"\"\n",
    "        volume = self.load_nifti_volume(filepath)\n",
    "        return self.normalize(volume)\n",
    "    \n",
    "    def normalize(self, volume):\n",
    "        \"\"\"Normalize volume to have zero mean and unit variance (only for non-zero voxels)\"\"\"\n",
    "        mask = volume > 0\n",
    "        if np.sum(mask) > 0:\n",
    "            mean = np.mean(volume[mask])\n",
    "            std = np.std(volume[mask])\n",
    "            if std > 0:\n",
    "                volume[mask] = (volume[mask] - mean) / std\n",
    "        return volume\n",
    "    \n",
    "    def random_crop_3d(self, image, mask):\n",
    "        \"\"\"Random crop a 3D patch\"\"\"\n",
    "        c, h, w, d = image.shape\n",
    "        ph, pw, pd = self.patch_size\n",
    "        \n",
    "        # If image is smaller than patch size, pad it\n",
    "        if h < ph or w < pw or d < pd:\n",
    "            # Calculate padding sizes\n",
    "            pad_h = max(0, ph - h)\n",
    "            pad_w = max(0, pw - w)\n",
    "            pad_d = max(0, pd - d)\n",
    "            \n",
    "            # Pad image and mask\n",
    "            image_pad = np.pad(image, ((0, 0), (0, pad_h), (0, pad_w), (0, pad_d)), mode='constant')\n",
    "            mask_pad = np.pad(mask, ((0, 0), (0, pad_h), (0, pad_w), (0, pad_d)), mode='constant')\n",
    "            \n",
    "            h, w, d = h + pad_h, w + pad_w, d + pad_d\n",
    "            image = image_pad\n",
    "            mask = mask_pad\n",
    "        \n",
    "        # Get random start indices\n",
    "        h_start = np.random.randint(0, h - ph + 1)\n",
    "        w_start = np.random.randint(0, w - pw + 1)\n",
    "        d_start = np.random.randint(0, d - pd + 1)\n",
    "        \n",
    "        # Extract patch\n",
    "        image_patch = image[:, h_start:h_start+ph, w_start:w_start+pw, d_start:d_start+pd]\n",
    "        mask_patch = mask[:, h_start:h_start+ph, w_start:w_start+pw, d_start:d_start+pd]\n",
    "        \n",
    "        return image_patch, mask_patch\n",
    "\n",
    "\n",
    "# Data augmentation classes\n",
    "class RandomFlip:\n",
    "    \"\"\"Random flip augmentation\"\"\"\n",
    "    def __init__(self, flip_prob=0.5):\n",
    "        self.flip_prob = flip_prob\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        # Flip along depth dimension (axis 3)\n",
    "        if np.random.rand() < self.flip_prob:\n",
    "            image = np.flip(image, axis=3).copy()\n",
    "            mask = np.flip(mask, axis=3).copy()\n",
    "        \n",
    "        # Flip along height dimension (axis 1)\n",
    "        if np.random.rand() < self.flip_prob:\n",
    "            image = np.flip(image, axis=1).copy()\n",
    "            mask = np.flip(mask, axis=1).copy()\n",
    "        \n",
    "        # Flip along width dimension (axis 2)\n",
    "        if np.random.rand() < self.flip_prob:\n",
    "            image = np.flip(image, axis=2).copy()\n",
    "            mask = np.flip(mask, axis=2).copy()\n",
    "        \n",
    "        return image, mask\n",
    "\n",
    "class RandomIntensityShift:\n",
    "    \"\"\"Random intensity shift augmentation\"\"\"\n",
    "    def __init__(self, shift_range=(-0.1, 0.1)):\n",
    "        self.shift_range = shift_range\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        for i in range(image.shape[0]):\n",
    "            shift = np.random.uniform(*self.shift_range)\n",
    "            image[i] = image[i] + shift\n",
    "        return image, mask\n",
    "\n",
    "class RandomIntensityScale:\n",
    "    \"\"\"Random intensity scale augmentation\"\"\"\n",
    "    def __init__(self, scale_range=(0.9, 1.1)):\n",
    "        self.scale_range = scale_range\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        for i in range(image.shape[0]):\n",
    "            scale = np.random.uniform(*self.scale_range)\n",
    "            image[i] = image[i] * scale\n",
    "        return image, mask\n",
    "\n",
    "class Compose:\n",
    "    \"\"\"Compose multiple transforms\"\"\"\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __call__(self, image, mask):\n",
    "        for transform in self.transforms:\n",
    "            image, mask = transform(image, mask)\n",
    "        return image, mask\n",
    "def create_dataloaders(data_dir, batch_size=1, patch_size=(128, 128, 128), train_ratio=0.8, num_workers=4, seed=42):\n",
    "    \"\"\"Create train and validation dataloaders for BraTS 2021 dataset\"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = Compose([\n",
    "        RandomFlip(flip_prob=0.5),\n",
    "        RandomIntensityShift(shift_range=(-0.1, 0.1)),\n",
    "        RandomIntensityScale(scale_range=(0.9, 1.1))\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset\n",
    "    full_dataset = BraTS2021Dataset(\n",
    "        data_dir=data_dir,\n",
    "        patch_size=patch_size,\n",
    "        transform=None,  # We'll apply transforms later for train/val\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Split into train and validation\n",
    "    dataset_size = len(full_dataset)\n",
    "    train_size = int(train_ratio * dataset_size)\n",
    "    val_size = dataset_size - train_size\n",
    "    \n",
    "    print(f\"Total dataset size: {dataset_size}\")\n",
    "    print(f\"Training set size: {train_size}\")\n",
    "    print(f\"Validation set size: {val_size}\")\n",
    "    \n",
    "    if dataset_size == 0:\n",
    "        raise ValueError(\"Dataset is empty. Please check your data path and structure.\")\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_dataset, [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "    # Create custom Dataset classes to apply different transforms\n",
    "    class TransformDataset(Dataset):\n",
    "        def __init__(self, dataset, transform=None):\n",
    "            self.dataset = dataset\n",
    "            self.transform = transform\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.dataset)\n",
    "            \n",
    "        def __getitem__(self, idx):\n",
    "            image, mask = self.dataset[idx]\n",
    "            \n",
    "            # Convert torch tensors back to numpy for transforms\n",
    "            image_np = image.numpy()\n",
    "            mask_np = mask.numpy()\n",
    "            \n",
    "            if self.transform:\n",
    "                image_np, mask_np = self.transform(image_np, mask_np)\n",
    "                \n",
    "            return torch.from_numpy(image_np), torch.from_numpy(mask_np)\n",
    "    \n",
    "    # Apply transforms\n",
    "    train_dataset_with_transform = TransformDataset(train_dataset, train_transform)\n",
    "    val_dataset_with_transform = TransformDataset(val_dataset, None)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset_with_transform,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset_with_transform,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1290ce3a-4c65-42ff-8287-2eb42d5444d2",
   "metadata": {},
   "source": [
    "# model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60257410-517b-4466-a801-07ce24d889bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Sequence, Tuple, Type, Union\n",
    "\n",
    "def ensure_tuple_rep(val, dim):\n",
    "    \"\"\"\n",
    "    Ensures input is a tuple of length dim by repeating the value if needed\n",
    "    \"\"\"\n",
    "    if isinstance(val, (list, tuple)):\n",
    "        if len(val) == dim:\n",
    "            return tuple(val)\n",
    "        else:\n",
    "            raise ValueError(f\"Length of input {len(val)} doesn't match requested length {dim}\")\n",
    "    else:\n",
    "        return tuple(val for _ in range(dim))\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    \"\"\"Tensor initialization with truncated normal distribution\"\"\"\n",
    "    def norm_cdf(x):\n",
    "        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "        tensor.erfinv_()\n",
    "        tensor.mul_(std * math.sqrt(2.0))\n",
    "        tensor.add_(mean)\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n",
    "    \"\"\"Truncated normal distribution initialization\"\"\"\n",
    "    if not std > 0:\n",
    "        raise ValueError(\"Standard deviation should be greater than zero.\")\n",
    "    if a >= b:\n",
    "        raise ValueError(\"Minimum cutoff value (a) should be smaller than maximum cutoff value (b).\")\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Stochastic drop paths per sample for residual blocks\"\"\"\n",
    "    def __init__(self, drop_prob=0.0, scale_by_keep=True):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "        \n",
    "        if not (0 <= drop_prob <= 1):\n",
    "            raise ValueError(\"Drop path prob should be between 0 and 1.\")\n",
    "            \n",
    "    def drop_path(self, x, drop_prob=0.0, training=False, scale_by_keep=True):\n",
    "        if drop_prob == 0.0 or not training:\n",
    "            return x\n",
    "        keep_prob = 1 - drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "        if keep_prob > 0.0 and scale_by_keep:\n",
    "            random_tensor.div_(keep_prob)\n",
    "        return x * random_tensor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Image to Patch Embedding implementation\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, patch_size, in_chans, embed_dim, norm_layer=None, spatial_dims=3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "        self.spatial_dims = spatial_dims\n",
    "        \n",
    "        if spatial_dims == 3:\n",
    "            self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        elif spatial_dims == 2:\n",
    "            self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        else:\n",
    "            raise ValueError(\"spatial_dims must be 2 or 3\")\n",
    "            \n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        if self.norm is not None:\n",
    "            if self.spatial_dims == 3:\n",
    "                x = x.permute(0, 2, 3, 4, 1)\n",
    "                x = self.norm(x)\n",
    "                x = x.permute(0, 4, 1, 2, 3)\n",
    "            else:\n",
    "                x = x.permute(0, 2, 3, 1)\n",
    "                x = self.norm(x)\n",
    "                x = x.permute(0, 3, 1, 2)\n",
    "        return x\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"MLP Block\"\"\"\n",
    "    def __init__(self, hidden_size, mlp_dim, dropout_rate=0.0, act=\"GELU\", dropout_mode=\"swin\"):\n",
    "        super().__init__()\n",
    "        if act == \"GELU\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif act == \"ReLU\":\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {act}\")\n",
    "            \n",
    "        self.dropout_mode = dropout_mode\n",
    "        self.linear1 = nn.Linear(hidden_size, mlp_dim)\n",
    "        self.linear2 = nn.Linear(mlp_dim, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        if self.dropout_mode == \"swin\":\n",
    "            x = self.dropout(x)\n",
    "            x = self.linear2(x)\n",
    "            x = self.dropout(x)\n",
    "        else:\n",
    "            x = self.linear2(x)\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Window partition operation\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        window_size: local window size\n",
    "    \"\"\"\n",
    "    x_shape = x.size()\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            window_size[0],\n",
    "            h // window_size[1],\n",
    "            window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[2],\n",
    "            c,\n",
    "        )\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n",
    "        )\n",
    "    elif len(x_shape) == 4:\n",
    "        b, h, w, c = x.shape\n",
    "        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, dims):\n",
    "    \"\"\"\n",
    "    Window reverse operation\n",
    "    Args:\n",
    "        windows: windows tensor\n",
    "        window_size: local window size\n",
    "        dims: dimension values\n",
    "    \"\"\"\n",
    "    if len(dims) == 4:\n",
    "        b, d, h, w = dims\n",
    "        x = windows.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            h // window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[0],\n",
    "            window_size[1],\n",
    "            window_size[2],\n",
    "            -1,\n",
    "        )\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
    "    elif len(dims) == 3:\n",
    "        b, h, w = dims\n",
    "        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
    "    return x\n",
    "\n",
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    \"\"\"\n",
    "    Computing window size based on input size\n",
    "    \"\"\"\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        use_shift_size = list(shift_size)\n",
    "    for i in range(len(x_size)):\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"\n",
    "    Computing region masks\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    \n",
    "    if len(dims) == 3:\n",
    "        d, h, w = dims\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "    elif len(dims) == 2:\n",
    "        h, w = dims\n",
    "        img_mask = torch.zeros((1, h, w, 1), device=device)\n",
    "        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "                \n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "    \n",
    "    return attn_mask\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        qkv_bias=False,\n",
    "        attn_drop=0.0,\n",
    "        proj_drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        \n",
    "        if len(self.window_size) == 3:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            coords_d = torch.arange(self.window_size[0])\n",
    "            coords_h = torch.arange(self.window_size[1])\n",
    "            coords_w = torch.arange(self.window_size[2])\n",
    "            coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "        elif len(self.window_size) == 2:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "            )\n",
    "            coords_h = torch.arange(self.window_size[0])\n",
    "            coords_w = torch.arange(self.window_size[1])\n",
    "            coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "            \n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        \n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index[:n, :n].reshape(-1)\n",
    "        ].reshape(n, n, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        \n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]\n",
    "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "            \n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer Block\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        shift_size,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=\"GELU\",\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        if min(self.window_size) <= min(self.shift_size):\n",
    "            self.shift_size = tuple(0 for i in self.window_size)\n",
    "            \n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        \n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = MLPBlock(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
    "        \n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = pad_d0 = 0\n",
    "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n",
    "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n",
    "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "            _, dp, hp, wp, _ = x.shape\n",
    "            dims = [b, dp, hp, wp]\n",
    "        elif len(x_shape) == 4:\n",
    "            b, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = 0\n",
    "            pad_r = (window_size[0] - h % window_size[0]) % window_size[0]\n",
    "            pad_b = (window_size[1] - w % window_size[1]) % window_size[1]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, hp, wp, _ = x.shape\n",
    "            dims = [b, hp, wp]\n",
    "            \n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "            \n",
    "        x_windows = window_partition(shifted_x, window_size)\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
    "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
    "        \n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "            \n",
    "        if len(x_shape) == 5:\n",
    "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :d, :h, :w, :].contiguous()\n",
    "        elif len(x_shape) == 4:\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :h, :w, :].contiguous()\n",
    "                \n",
    "        return x\n",
    "        \n",
    "    def forward_part2(self, x):\n",
    "        return self.drop_path(self.mlp(self.norm2(x)))\n",
    "        \n",
    "    def forward(self, x, mask_matrix):\n",
    "        shortcut = x\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix)\n",
    "        else:\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x)\n",
    "        else:\n",
    "            x = x + self.forward_part2(x)\n",
    "        return x\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch merging layer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm, spatial_dims=3):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if spatial_dims == 3:\n",
    "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(8 * dim)\n",
    "        elif spatial_dims == 2:\n",
    "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(4 * dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, d % 2, 0, w % 2, 0, h % 2))\n",
    "            x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "            x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "            x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "            x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "            x4 = x[:, 1::2, 0::2, 1::2, :]\n",
    "            x5 = x[:, 0::2, 1::2, 0::2, :]\n",
    "            x6 = x[:, 0::2, 0::2, 1::2, :]\n",
    "            x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "            x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        elif len(x_shape) == 4:\n",
    "            b, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n",
    "            x0 = x[:, 0::2, 0::2, :]\n",
    "            x1 = x[:, 1::2, 0::2, :]\n",
    "            x2 = x[:, 0::2, 1::2, :]\n",
    "            x3 = x[:, 1::2, 1::2, :]\n",
    "            x = torch.cat([x0, x1, x2, x3], -1)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Swin Transformer layer in one stage\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        window_size,\n",
    "        drop_path,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        downsample=None,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)\n",
    "        self.no_shift = tuple(0 for i in window_size)\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=self.window_size,\n",
    "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    use_checkpoint=use_checkpoint,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        if self.downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, c, d, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            x = x.permute(0, 2, 3, 4, 1)  # (B, D, H, W, C)\n",
    "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
    "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
    "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
    "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
    "            \n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "                \n",
    "            x = x.view(b, d, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = x.permute(0, 4, 1, 2, 3)  # (B, C, D, H, W)\n",
    "            \n",
    "        elif len(x_shape) == 4:\n",
    "            b, c, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            x = x.permute(0, 2, 3, 1)  # (B, H, W, C)\n",
    "            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n",
    "            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n",
    "            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n",
    "            \n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "                \n",
    "            x = x.view(b, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = x.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer backbone\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans,\n",
    "        embed_dim,\n",
    "        window_size,\n",
    "        patch_size,\n",
    "        depths,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        drop_path_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        patch_norm=False,\n",
    "        use_checkpoint=False,\n",
    "        spatial_dims=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.window_size = window_size\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=self.patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        \n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        \n",
    "        self.layers1 = nn.ModuleList()\n",
    "        self.layers2 = nn.ModuleList()\n",
    "        self.layers3 = nn.ModuleList()\n",
    "        self.layers4 = nn.ModuleList()\n",
    "        \n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=self.window_size,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            \n",
    "            if i_layer == 0:\n",
    "                self.layers1.append(layer)\n",
    "            elif i_layer == 1:\n",
    "                self.layers2.append(layer)\n",
    "            elif i_layer == 2:\n",
    "                self.layers3.append(layer)\n",
    "            elif i_layer == 3:\n",
    "                self.layers4.append(layer)\n",
    "                \n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "        \n",
    "    def proj_out(self, x, normalize=False):\n",
    "        if normalize:\n",
    "            x_shape = x.size()\n",
    "            if len(x_shape) == 5:\n",
    "                n, ch, d, h, w = x_shape\n",
    "                x = x.permute(0, 2, 3, 4, 1)  # (n, d, h, w, ch)\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = x.permute(0, 4, 1, 2, 3)  # (n, ch, d, h, w)\n",
    "            elif len(x_shape) == 4:\n",
    "                n, ch, h, w = x_shape\n",
    "                x = x.permute(0, 2, 3, 1)  # (n, h, w, ch)\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = x.permute(0, 3, 1, 2)  # (n, ch, h, w)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x, normalize=True):\n",
    "        x0 = self.patch_embed(x)\n",
    "        x0 = self.pos_drop(x0)\n",
    "        x0_out = self.proj_out(x0, normalize)\n",
    "        x1 = self.layers1[0](x0.contiguous())\n",
    "        x1_out = self.proj_out(x1, normalize)\n",
    "        x2 = self.layers2[0](x1.contiguous())\n",
    "        x2_out = self.proj_out(x2, normalize)\n",
    "        x3 = self.layers3[0](x2.contiguous())\n",
    "        x3_out = self.proj_out(x3, normalize)\n",
    "        x4 = self.layers4[0](x3.contiguous())\n",
    "        x4_out = self.proj_out(x4, normalize)\n",
    "        \n",
    "        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n",
    "\n",
    "class UnetrBasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A basic block for UNETR with debug logs\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        norm_name=\"instance\",\n",
    "        res_block=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        if norm_name == \"instance\":\n",
    "            if spatial_dims == 3:\n",
    "                self.norm = nn.InstanceNorm3d(out_channels)\n",
    "            elif spatial_dims == 2:\n",
    "                self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        elif norm_name == \"batch\":\n",
    "            if spatial_dims == 3:\n",
    "                self.norm = nn.BatchNorm3d(out_channels)\n",
    "            elif spatial_dims == 2:\n",
    "                self.norm = nn.BatchNorm2d(out_channels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported normalization: {norm_name}\")\n",
    "            \n",
    "        if spatial_dims == 3:\n",
    "            self.conv1 = nn.Conv3d(\n",
    "                in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2\n",
    "            )\n",
    "            if res_block:\n",
    "                self.conv2 = nn.Conv3d(\n",
    "                    out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2\n",
    "                )\n",
    "                self.conv3 = nn.Conv3d(\n",
    "                    out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2\n",
    "                )\n",
    "                \n",
    "            if stride > 1:\n",
    "                self.residual = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "            elif in_channels != out_channels:\n",
    "                self.residual = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            else:\n",
    "                self.residual = nn.Identity()\n",
    "        elif spatial_dims == 2:\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=(kernel_size - 1) // 2\n",
    "            )\n",
    "            if res_block:\n",
    "                self.conv2 = nn.Conv2d(\n",
    "                    out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2\n",
    "                )\n",
    "                self.conv3 = nn.Conv2d(\n",
    "                    out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=(kernel_size - 1) // 2\n",
    "                )\n",
    "                \n",
    "            if stride > 1:\n",
    "                self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "            elif in_channels != out_channels:\n",
    "                self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "            else:\n",
    "                self.residual = nn.Identity()\n",
    "                \n",
    "        self.res_block = res_block\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f\"UnetrBasicBlock - Input shape: {x.shape}, in_channels: {self.in_channels}, out_channels: {self.out_channels}\")\n",
    "        \n",
    "        res = self.residual(x)\n",
    "        # print(f\"UnetrBasicBlock - After residual shape: {res.shape}\")\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        # print(f\"UnetrBasicBlock - After conv1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        if self.res_block:\n",
    "            x = self.conv2(x)\n",
    "            # print(f\"UnetrBasicBlock - After conv2 shape: {x.shape}\")\n",
    "            x = self.norm(x)\n",
    "            x = self.activation(x)\n",
    "            \n",
    "            x = self.conv3(x)\n",
    "            # print(f\"UnetrBasicBlock - After conv3 shape: {x.shape}\")\n",
    "            x = self.norm(x)\n",
    "            \n",
    "        x = x + res\n",
    "        x = self.activation(x)\n",
    "        # print(f\"UnetrBasicBlock - Output shape: {x.shape}\")\n",
    "        return x\n",
    "class UnetrUpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling module for UNETR with debug logs\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        spatial_dims,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        upsample_kernel_size,\n",
    "        norm_name=\"instance\",\n",
    "        res_block=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        if spatial_dims == 3:\n",
    "            self.transp_conv = nn.ConvTranspose3d(\n",
    "                in_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_kernel_size\n",
    "            )\n",
    "        elif spatial_dims == 2:\n",
    "            self.transp_conv = nn.ConvTranspose2d(\n",
    "                in_channels, out_channels, kernel_size=upsample_kernel_size, stride=upsample_kernel_size\n",
    "            )\n",
    "            \n",
    "        # The important fix: we need to account for the concatenated channels\n",
    "        # Previously we used in_channels + out_channels which led to a channel mismatch\n",
    "        self.conv_block = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=out_channels * 2,  # This is after concatenation\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=res_block,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        # Add debug info\n",
    "        # print(f\"UnetrUpBlock - Input x shape: {x.shape}, skip shape: {skip.shape}\")\n",
    "        \n",
    "        x = self.transp_conv(x)\n",
    "        # print(f\"UnetrUpBlock - After transpose conv shape: {x.shape}\")\n",
    "        \n",
    "        x = torch.cat((x, skip), dim=1)\n",
    "        # print(f\"UnetrUpBlock - After concatenation shape: {x.shape}\")\n",
    "        \n",
    "        x = self.conv_block(x)\n",
    "        # print(f\"UnetrUpBlock - Output shape: {x.shape}\")\n",
    "        return x\n",
    "class UnetOutBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A final output block for UNETR\n",
    "    \"\"\"\n",
    "    def __init__(self, spatial_dims, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        if spatial_dims == 3:\n",
    "            self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        elif spatial_dims == 2:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class SwinUNETR(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin UNETR based on: \"Hatamizadeh et al.,\n",
    "    Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images\n",
    "    <https://arxiv.org/abs/2201.01266>\"\n",
    "    With added debug logging\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        depths=(2, 2, 2, 2),\n",
    "        num_heads=(3, 6, 12, 24),\n",
    "        feature_size=24,\n",
    "        norm_name=\"instance\",\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        dropout_path_rate=0.0,\n",
    "        normalize=True,\n",
    "        use_checkpoint=False,\n",
    "        spatial_dims=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Print configuration for debugging\n",
    "        # print(f\"SwinUNETR - Initializing with img_size={img_size}, in_channels={in_channels}, out_channels={out_channels}\")\n",
    "        # print(f\"SwinUNETR - feature_size={feature_size}, depths={depths}, num_heads={num_heads}\")\n",
    "        \n",
    "        img_size = ensure_tuple_rep(img_size, spatial_dims)\n",
    "        patch_size = ensure_tuple_rep(2, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(7, spatial_dims)\n",
    "        \n",
    "        if not (spatial_dims == 2 or spatial_dims == 3):\n",
    "            raise ValueError(\"spatial dimension should be 2 or 3.\")\n",
    "            \n",
    "        for m, p in zip(img_size, patch_size):\n",
    "            for i in range(5):\n",
    "                if m % np.power(p, i + 1) != 0:\n",
    "                    raise ValueError(\"input image size (img_size) should be divisible by stage-wise image resolution.\")\n",
    "                    \n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"dropout rate should be between 0 and 1.\")\n",
    "            \n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n",
    "            \n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"drop path rate should be between 0 and 1.\")\n",
    "            \n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"feature_size should be divisible by 12.\")\n",
    "            \n",
    "        self.normalize = normalize\n",
    "        \n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_size,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=4.0,\n",
    "            qkv_bias=True,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        \n",
    "        self.encoder1 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=in_channels,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.encoder2 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.encoder3 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=2 * feature_size,\n",
    "            out_channels=2 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.encoder4 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=4 * feature_size,\n",
    "            out_channels=4 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.encoder10 = UnetrBasicBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=16 * feature_size,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        # Fix the dimensions for decoder blocks\n",
    "        self.decoder5 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=8 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.decoder4 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=8 * feature_size,  # Changed from 'feature_size * 8'\n",
    "            out_channels=4 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.decoder3 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=4 * feature_size,\n",
    "            out_channels=2 * feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.decoder2 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=2 * feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.decoder1 = UnetrUpBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            kernel_size=3,\n",
    "            upsample_kernel_size=2,\n",
    "            norm_name=norm_name,\n",
    "            res_block=True,\n",
    "        )\n",
    "        \n",
    "        self.out = UnetOutBlock(\n",
    "            spatial_dims=spatial_dims,\n",
    "            in_channels=feature_size,\n",
    "            out_channels=out_channels\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        # Print input shape\n",
    "        # print(f\"SwinUNETR - Input shape: {x_in.shape}\")\n",
    "        \n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "        \n",
    "        # Print shapes for debugging\n",
    "        # for i, hidden_state in enumerate(hidden_states_out):\n",
    "            # print(f\"SwinUNETR - Hidden state {i} shape: {hidden_state.shape}\")\n",
    "        \n",
    "        enc0 = self.encoder1(x_in)\n",
    "        # print(f\"SwinUNETR - enc0 shape: {enc0.shape}\")\n",
    "        \n",
    "        enc1 = self.encoder2(hidden_states_out[0])\n",
    "        # print(f\"SwinUNETR - enc1 shape: {enc1.shape}\")\n",
    "        \n",
    "        enc2 = self.encoder3(hidden_states_out[1])\n",
    "        # print(f\"SwinUNETR - enc2 shape: {enc2.shape}\")\n",
    "        \n",
    "        enc3 = self.encoder4(hidden_states_out[2])\n",
    "        # print(f\"SwinUNETR - enc3 shape: {enc3.shape}\")\n",
    "        \n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        # print(f\"SwinUNETR - dec4 (bottleneck) shape: {dec4.shape}\")\n",
    "        \n",
    "        dec3 = self.decoder5(dec4, hidden_states_out[3])\n",
    "        # print(f\"SwinUNETR - dec3 shape: {dec3.shape}\")\n",
    "        \n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        # print(f\"SwinUNETR - dec2 shape: {dec2.shape}\")\n",
    "        \n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        # print(f\"SwinUNETR - dec1 shape: {dec1.shape}\")\n",
    "        \n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "        # print(f\"SwinUNETR - dec0 shape: {dec0.shape}\")\n",
    "        \n",
    "        out = self.decoder1(dec0, enc0)\n",
    "        # print(f\"SwinUNETR - Final decoder output shape: {out.shape}\")\n",
    "        \n",
    "        logits = self.out(out)\n",
    "        # print(f\"SwinUNETR - Final output shape: {logits.shape}\")\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a213e-5fec-45de-8912-70b41b4fb1bb",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13a4fff-4a7d-426b-85e1-9c1516c2c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient_for_metrics(y_pred, y_true, smooth=1e-5, ignore_empty=True):\n",
    "    \"\"\"\n",
    "    Calculate Dice coefficient for metrics reporting (with thresholding)\n",
    "    This version detaches gradients and is used for metrics, not loss\n",
    "    \"\"\"\n",
    "    # Apply sigmoid and threshold - note this breaks gradients but is OK for metrics\n",
    "    y_pred = (torch.sigmoid(y_pred) > 0.5).float()\n",
    "    \n",
    "    # Compute for each class\n",
    "    batch_size = y_pred.shape[0]\n",
    "    num_classes = y_pred.shape[1]\n",
    "    dice_scores = torch.zeros(batch_size, num_classes, device=y_pred.device)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for c in range(num_classes):\n",
    "            # Get current class\n",
    "            pred = y_pred[b, c, ...].bool()  # Binary prediction\n",
    "            true = y_true[b, c, ...].bool()  # Binary ground truth\n",
    "            \n",
    "            # Count positive pixels\n",
    "            true_sum = torch.sum(true)\n",
    "            \n",
    "            if true_sum > 0:\n",
    "                # Compute intersection (only count pixels where both are True)\n",
    "                intersection = torch.sum(torch.masked_select(true, pred))\n",
    "                # Compute Dice\n",
    "                dice_scores[b, c] = (2.0 * intersection) / (true_sum + torch.sum(pred))\n",
    "            else:\n",
    "                # Handle empty ground truth\n",
    "                if ignore_empty:\n",
    "                    dice_scores[b, c] = float('nan')\n",
    "                else:\n",
    "                    pred_sum = torch.sum(pred)\n",
    "                    if pred_sum <= 0:\n",
    "                        dice_scores[b, c] = 1.0  # Both empty - perfect match\n",
    "                    else:\n",
    "                        dice_scores[b, c] = 0.0  # Empty ground truth but prediction has values\n",
    "    \n",
    "    return dice_scores\n",
    "\n",
    "def dice_loss_function(y_pred, y_true, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Differentiable Dice loss function for training\n",
    "    Uses soft Dice which preserves gradients\n",
    "    \"\"\"\n",
    "    # Apply sigmoid but NO thresholding to maintain gradients\n",
    "    y_pred_sigmoid = torch.sigmoid(y_pred)\n",
    "    \n",
    "    # Reshape for reduction\n",
    "    batch_size = y_pred.shape[0]\n",
    "    num_classes = y_pred.shape[1]\n",
    "    \n",
    "    # Flatten prediction and target tensors for easier operations\n",
    "    y_pred_flat = y_pred_sigmoid.view(batch_size, num_classes, -1)\n",
    "    y_true_flat = y_true.view(batch_size, num_classes, -1)\n",
    "    \n",
    "    # Calculate intersection and union using differentiable operations\n",
    "    intersection = torch.sum(y_pred_flat * y_true_flat, dim=2)\n",
    "    pred_sum = torch.sum(y_pred_flat, dim=2)\n",
    "    true_sum = torch.sum(y_true_flat, dim=2)\n",
    "    \n",
    "    # Calculate dice coefficient with smoothing\n",
    "    dice = (2. * intersection + smooth) / (pred_sum + true_sum + smooth)\n",
    "    \n",
    "    # Convert to loss (1 - Dice)\n",
    "    loss = 1.0 - dice\n",
    "    \n",
    "    # Handle empty ground truth cases\n",
    "    empty_gt = (true_sum == 0)\n",
    "    if torch.any(empty_gt):\n",
    "        # Set loss to 0 for empty ground truth if prediction is also empty\n",
    "        empty_pred = (pred_sum < smooth)\n",
    "        loss[empty_gt & empty_pred] = 0.0\n",
    "    \n",
    "    # Return mean loss\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def multiclass_dice_loss(y_pred, y_true, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Multiclass Dice loss that preserves gradients for backpropagation\n",
    "    \"\"\"\n",
    "    return dice_loss_function(y_pred, y_true, smooth)\n",
    "\n",
    "def multiclass_dice_coefficient(y_pred, y_true, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate average Dice coefficient across all classes for metrics reporting\n",
    "    \"\"\"\n",
    "    # Calculate per-class Dice (using the non-differentiable version for metrics)\n",
    "    dice_scores = dice_coefficient_for_metrics(y_pred, y_true, smooth)\n",
    "    \n",
    "    # Average over batches for each class\n",
    "    class_scores = torch.nanmean(dice_scores, dim=0)  # Shape: [C]\n",
    "    \n",
    "    # Average over classes for mean Dice\n",
    "    overall_dice = torch.nanmean(class_scores)\n",
    "    \n",
    "    # Replace NaNs with 1.0 for printing (assuming perfect score for empty regions)\n",
    "    display_scores = class_scores.clone()\n",
    "    display_scores = torch.where(torch.isnan(display_scores), torch.ones_like(display_scores), display_scores)\n",
    "    \n",
    "    et_score = display_scores[0].item()\n",
    "    wt_score = display_scores[1].item()\n",
    "    tc_score = display_scores[2].item()\n",
    "    \n",
    "    # For overall display, use the nanmean but replace NaN with 1.0\n",
    "    overall_display = overall_dice.item() if not torch.isnan(overall_dice) else 1.0\n",
    "    \n",
    "    print(f\"Class Dice - ET: {et_score:.4f}, WT: {wt_score:.4f}, TC: {tc_score:.4f}, Avg: {overall_display:.4f}\")\n",
    "    \n",
    "    return overall_dice\n",
    "\n",
    "def class_wise_dice_coefficient(y_pred, y_true, smooth=1e-5):\n",
    "    \"\"\"\n",
    "    Calculate class-wise Dice scores for reporting\n",
    "    \"\"\"\n",
    "    # Calculate per-class Dice (using the non-differentiable version for metrics)\n",
    "    dice_scores = dice_coefficient_for_metrics(y_pred, y_true, smooth)\n",
    "    \n",
    "    # Average over batches for each class\n",
    "    class_scores = torch.nanmean(dice_scores, dim=0)  # Shape: [C]\n",
    "    \n",
    "    # Replace NaNs with 1.0 for class-wise metrics\n",
    "    class_scores = torch.where(torch.isnan(class_scores), torch.ones_like(class_scores), class_scores)\n",
    "    \n",
    "    # Convert to list\n",
    "    return class_scores.cpu().detach().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4daf59-2d30-406b-90bc-f46e3b95ace6",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa46759d-7279-4d8f-814c-4ec1424b1e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs=800, save_dir='checkpoints'):\n",
    "    \"\"\"Train the Swin UNETR model with class-wise Dice score tracking\"\"\"\n",
    "\n",
    "\n",
    "    import time\n",
    "    import os\n",
    "    import math\n",
    "    import torch\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_val_dice = 0.0\n",
    "    \n",
    "    # For tracking metrics history\n",
    "    history = {\n",
    "        'train_loss': [], 'train_dice': [], 'train_dice_et': [], 'train_dice_wt': [], 'train_dice_tc': [],\n",
    "        'val_loss': [], 'val_dice': [], 'val_dice_et': [], 'val_dice_wt': [], 'val_dice_tc': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_dice = 0\n",
    "        train_class_dice = {'ET': 0, 'WT': 0, 'TC': 0}\n",
    "        \n",
    "        # For handling NaN values properly\n",
    "        train_dice_values = []\n",
    "        train_class_dice_values = {'ET': [], 'WT': [], 'TC': []}\n",
    "        \n",
    "        train_start = time.time()\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = multiclass_dice_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate Dice score (detach to avoid unnecessary gradient computation)\n",
    "            with torch.no_grad():\n",
    "                dice_score = multiclass_dice_coefficient(outputs.detach(), targets)\n",
    "                \n",
    "                # Calculate class-wise Dice scores (ET, WT, TC)\n",
    "                class_scores = class_wise_dice_coefficient(outputs.detach(), targets)\n",
    "                \n",
    "                # Store values for proper averaging (handling NaN)\n",
    "                if not torch.isnan(dice_score):\n",
    "                    train_dice_values.append(dice_score.item())\n",
    "                \n",
    "                train_class_dice_values['ET'].append(class_scores[0])\n",
    "                train_class_dice_values['WT'].append(class_scores[1])\n",
    "                train_class_dice_values['TC'].append(class_scores[2])\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_dice += dice_score.item() if not torch.isnan(dice_score) else 0\n",
    "            \n",
    "            # For tracking running average (needed for batch display)\n",
    "            train_class_dice['ET'] += class_scores[0] if not math.isnan(class_scores[0]) else 0\n",
    "            train_class_dice['WT'] += class_scores[1] if not math.isnan(class_scores[1]) else 0\n",
    "            train_class_dice['TC'] += class_scores[2] if not math.isnan(class_scores[2]) else 0\n",
    "            \n",
    "            # Print status\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                batch_dice = dice_score.item() if not torch.isnan(dice_score) else 0\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Dice: {batch_dice:.4f}')\n",
    "        \n",
    "        train_time = time.time() - train_start\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Properly average Dice scores ignoring NaNs\n",
    "        train_dice = float(torch.nanmean(torch.tensor(train_dice_values))) if train_dice_values else 0.0\n",
    "        \n",
    "        # Average class-wise scores properly\n",
    "        for key in train_class_dice:\n",
    "            values = [v for v in train_class_dice_values[key] if not math.isnan(v)]\n",
    "            train_class_dice[key] = sum(values) / len(values) if values else 1.0  # Use 1.0 for empty regions\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_dice = 0\n",
    "        val_class_dice = {'ET': 0, 'WT': 0, 'TC': 0}\n",
    "        \n",
    "        # For handling NaN values properly\n",
    "        val_dice_values = []\n",
    "        val_class_dice_values = {'ET': [], 'WT': [], 'TC': []}\n",
    "        \n",
    "        val_start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = multiclass_dice_loss(outputs, targets)\n",
    "                dice_score = multiclass_dice_coefficient(outputs, targets)\n",
    "                \n",
    "                # Calculate class-wise Dice scores (ET, WT, TC)\n",
    "                class_scores = class_wise_dice_coefficient(outputs, targets)\n",
    "                \n",
    "                # Store values for proper averaging (handling NaN)\n",
    "                if not torch.isnan(dice_score):\n",
    "                    val_dice_values.append(dice_score.item())\n",
    "                \n",
    "                val_class_dice_values['ET'].append(class_scores[0])\n",
    "                val_class_dice_values['WT'].append(class_scores[1])\n",
    "                val_class_dice_values['TC'].append(class_scores[2])\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_dice += dice_score.item() if not torch.isnan(dice_score) else 0\n",
    "                \n",
    "                # For tracking running average\n",
    "                val_class_dice['ET'] += class_scores[0] if not math.isnan(class_scores[0]) else 0\n",
    "                val_class_dice['WT'] += class_scores[1] if not math.isnan(class_scores[1]) else 0\n",
    "                val_class_dice['TC'] += class_scores[2] if not math.isnan(class_scores[2]) else 0\n",
    "        \n",
    "        val_time = time.time() - val_start\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Properly average Dice scores ignoring NaNs\n",
    "        val_dice = float(torch.nanmean(torch.tensor(val_dice_values))) if val_dice_values else 0.0\n",
    "        \n",
    "        # Average class-wise scores properly\n",
    "        for key in val_class_dice:\n",
    "            values = [v for v in val_class_dice_values[key] if not math.isnan(v)]\n",
    "            val_class_dice[key] = sum(values) / len(values) if values else 1.0  # Use 1.0 for empty regions\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Print status with class-wise scores\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}, '\n",
    "              f'Train ET: {train_class_dice[\"ET\"]:.4f}, Train WT: {train_class_dice[\"WT\"]:.4f}, Train TC: {train_class_dice[\"TC\"]:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Dice: {val_dice:.4f}, '\n",
    "              f'Val ET: {val_class_dice[\"ET\"]:.4f}, Val WT: {val_class_dice[\"WT\"]:.4f}, Val TC: {val_class_dice[\"TC\"]:.4f}, '\n",
    "              f'Train Time: {train_time:.2f}s, Val Time: {val_time:.2f}s')\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['train_dice_et'].append(train_class_dice['ET'])\n",
    "        history['train_dice_wt'].append(train_class_dice['WT'])\n",
    "        history['train_dice_tc'].append(train_class_dice['TC'])\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_dice'].append(val_dice)\n",
    "        history['val_dice_et'].append(val_class_dice['ET'])\n",
    "        history['val_dice_wt'].append(val_class_dice['WT'])\n",
    "        history['val_dice_tc'].append(val_class_dice['TC'])\n",
    "        \n",
    "        # Save best model based on validation Dice score\n",
    "        if val_dice > best_val_dice:\n",
    "            best_val_dice = val_dice\n",
    "            model_path = os.path.join(save_dir, f'swin_unetr_best_dice.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_dice': train_dice,\n",
    "                'val_dice': val_dice,\n",
    "                'train_class_dice': train_class_dice,\n",
    "                'val_class_dice': val_class_dice,\n",
    "                'best_val_dice': best_val_dice,\n",
    "                'history': history\n",
    "            }, model_path)\n",
    "            print(f'Saved best model with validation Dice score: {best_val_dice:.4f}')\n",
    "        \n",
    "        # Also save based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_path = os.path.join(save_dir, f'swin_unetr_best_loss.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_dice': train_dice,\n",
    "                'val_dice': val_dice,\n",
    "                'train_class_dice': train_class_dice,\n",
    "                'val_class_dice': val_class_dice,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'history': history\n",
    "            }, model_path)\n",
    "            print(f'Saved best model with validation loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model_path = os.path.join(save_dir, f'swin_unetr_epoch_{epoch+1}.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_dice': train_dice,\n",
    "                'val_dice': val_dice,\n",
    "                'train_class_dice': train_class_dice,\n",
    "                'val_class_dice': val_class_dice,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_dice': best_val_dice,\n",
    "                'history': history\n",
    "            }, model_path)\n",
    "    \n",
    "    # Save final history for plotting\n",
    "    import json\n",
    "    # Convert float values to be JSON serializable\n",
    "    history_json = {k: [float(val) for val in v] for k, v in history.items()}\n",
    "    with open(os.path.join(save_dir, 'training_history.json'), 'w') as f:\n",
    "        json.dump(history_json, f)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81edb0b-4162-48be-b1a1-88f2c85f77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_config(img_size, feature_size, depths, num_heads):\n",
    "    \"\"\"\n",
    "    Validates the model configuration to catch potential issues early\n",
    "    \"\"\"\n",
    "    # Convert img_size to tuple if it's not already\n",
    "    if not isinstance(img_size, tuple):\n",
    "        img_size = tuple(ensure_tuple_rep(img_size, 3))\n",
    "    \n",
    "    patch_size = (2, 2, 2)\n",
    "    \n",
    "    # Check divisibility of image size by 2^(num_layers+1)\n",
    "    max_stages = len(depths) + 1  # +1 for initial patch embedding\n",
    "    for dim, ps in zip(img_size, patch_size):\n",
    "        for i in range(max_stages):\n",
    "            if dim % (ps ** i) != 0:\n",
    "                return False, f\"Image dimension {dim} is not divisible by {ps}^{i}\"\n",
    "    \n",
    "    # Check feature size divisibility\n",
    "    if feature_size % 12 != 0:\n",
    "        return False, f\"Feature size {feature_size} is not divisible by 12\"\n",
    "    \n",
    "    # Check lengths\n",
    "    if len(depths) != len(num_heads):\n",
    "        return False, f\"Length of depths {len(depths)} doesn't match length of num_heads {len(num_heads)}\"\n",
    "    \n",
    "    # Calculate expected tensor shapes\n",
    "    shapes = []\n",
    "    curr_shape = list(img_size)\n",
    "    \n",
    "    # Initial patch embedding divides dimensions by patch_size\n",
    "    for i in range(3):\n",
    "        curr_shape[i] = curr_shape[i] // patch_size[i]\n",
    "    \n",
    "    shapes.append(curr_shape.copy())\n",
    "    \n",
    "    # For each stage, dimensions are halved\n",
    "    for _ in range(len(depths)):\n",
    "        for i in range(3):\n",
    "            curr_shape[i] = curr_shape[i] // 2\n",
    "        shapes.append(curr_shape.copy())\n",
    "    \n",
    "    print(\"Expected feature map shapes:\")\n",
    "    for i, shape in enumerate(shapes):\n",
    "        feature_channels = feature_size * (2 ** min(i, len(depths)-1))\n",
    "        # print(f\"Stage {i}: {shape} with {feature_channels} channels\")\n",
    "    \n",
    "    return True, \"Configuration is valid\"\n",
    "    \n",
    "def main():\n",
    "    # Parameters\n",
    "    batch_size = 2\n",
    "    patch_size = (128,128,128)\n",
    "    feature_size = 48\n",
    "    depths = (2, 2, 2, 2)\n",
    "    num_heads = (3, 6, 12, 24)\n",
    "    lr = 0.001\n",
    "    num_epochs = 10\n",
    "    data_dir = \"workspace/data\"\n",
    "    save_dir = \"checkpoints_1\"\n",
    "    train_ratio = 0.8\n",
    "    \n",
    "    # Validate model configuration\n",
    "    valid, message = validate_model_config(patch_size, feature_size, depths, num_heads)\n",
    "    if not valid:\n",
    "        print(f\"ERROR - Invalid model configuration: {message}\")\n",
    "        print(\"Please adjust the model parameters before continuing.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Model configuration validated: {message}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        data_dir=data_dir, \n",
    "        batch_size=batch_size, \n",
    "        patch_size=patch_size, \n",
    "        train_ratio=train_ratio\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = SwinUNETR(\n",
    "        img_size=patch_size,\n",
    "        in_channels=4,  # T1, T1ce, T2, FLAIR\n",
    "        out_channels=3,  # ET, WT, TC\n",
    "        feature_size=feature_size,\n",
    "        depths=depths,\n",
    "        num_heads=num_heads,\n",
    "        norm_name=\"instance\",\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        dropout_path_rate=0.0,\n",
    "        normalize=True,\n",
    "        use_checkpoint=False,\n",
    "        spatial_dims=3\n",
    "    )\n",
    "    \n",
    "    # Print total parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total model parameters: {total_params:,}\")\n",
    "    \n",
    "    # Create optimizer and scheduler\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Train model\n",
    "    model,history = train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, save_dir)\n",
    "    \n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e8206-469a-4e13-8465-97502f475111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected feature map shapes:\n",
      "Model configuration validated: Configuration is valid\n",
      "Looking for data in: workspace/data\n",
      "Found 1251 subject directories\n",
      "Total dataset size: 1251\n",
      "Training set size: 1000\n",
      "Validation set size: 251\n",
      "Total model parameters: 84,842,997\n",
      "Class Dice - ET: 0.0153, WT: 0.0985, TC: 0.0422, Avg: 0.0520\n",
      "Class Dice - ET: 0.0291, WT: 0.1958, TC: 0.0714, Avg: 0.0987\n",
      "Class Dice - ET: 0.0025, WT: 0.1233, TC: 0.0116, Avg: 0.0458\n",
      "Class Dice - ET: 0.0053, WT: 0.2806, TC: 0.0176, Avg: 0.1012\n",
      "Class Dice - ET: 0.0300, WT: 0.5807, TC: 0.1050, Avg: 0.2386\n",
      "Class Dice - ET: 0.0123, WT: 0.3417, TC: 0.0744, Avg: 0.1428\n",
      "Class Dice - ET: 0.0098, WT: 0.1862, TC: 0.0379, Avg: 0.0780\n",
      "Class Dice - ET: 0.0000, WT: 0.2358, TC: 0.0002, Avg: 0.0787\n"
     ]
    }
   ],
   "source": [
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a1a70-1c8d-490e-af21-7835064323f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536c73f-7155-4514-8e12-d12093d427a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d1bb3-76bd-45cc-878f-3cd61016b653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0f431-74a7-4c1d-bad4-e59fe58d00f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a56c461-bc1c-4c01-a199-45f13617548d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95335e4b-efc4-4775-a210-efc3e8306cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_training_history(history_path):\n",
    "    \"\"\"\n",
    "    Load and visualize training history from JSON file\n",
    "    \n",
    "    Args:\n",
    "        history_path: Path to the training_history.json file\n",
    "    \"\"\"\n",
    "    # Load the history\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot loss\n",
    "    axs[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axs[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "    axs[0, 0].set_title('Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Plot average Dice score\n",
    "    axs[0, 1].plot(history['train_dice'], label='Train Dice')\n",
    "    axs[0, 1].plot(history['val_dice'], label='Val Dice')\n",
    "    axs[0, 1].set_title('Average Dice Score')\n",
    "    axs[0, 1].set_xlabel('Epoch')\n",
    "    axs[0, 1].set_ylabel('Dice Score')\n",
    "    axs[0, 1].legend()\n",
    "    \n",
    "    # Plot training class-wise Dice scores\n",
    "    axs[1, 0].plot(history['train_dice_et'], label='ET')\n",
    "    axs[1, 0].plot(history['train_dice_wt'], label='WT')\n",
    "    axs[1, 0].plot(history['train_dice_tc'], label='TC')\n",
    "    axs[1, 0].set_title('Training Class-wise Dice Scores')\n",
    "    axs[1, 0].set_xlabel('Epoch')\n",
    "    axs[1, 0].set_ylabel('Dice Score')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # Plot validation class-wise Dice scores\n",
    "    axs[1, 1].plot(history['val_dice_et'], label='ET')\n",
    "    axs[1, 1].plot(history['val_dice_wt'], label='WT')\n",
    "    axs[1, 1].plot(history['val_dice_tc'], label='TC')\n",
    "    axs[1, 1].set_title('Validation Class-wise Dice Scores')\n",
    "    axs[1, 1].set_xlabel('Epoch')\n",
    "    axs[1, 1].set_ylabel('Dice Score')\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_training_history('/workspace/checkpoints/training_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42824fe-ecb6-4cb0-9760-c12bbbfd7d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_test_model(model_path, val_loader, device=None):\n",
    "    \"\"\"\n",
    "    Load a trained model and test it on validation data\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model checkpoint\n",
    "        val_loader: Validation data loader\n",
    "        device: Device to run the model on (default: None, will use CUDA if available)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load the model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Print training info from checkpoint\n",
    "    print(f\"Model trained for {checkpoint['epoch']} epochs\")\n",
    "    print(f\"Best validation Dice score: {checkpoint.get('best_val_dice', 'N/A')}\")\n",
    "    \n",
    "    # Create a new model instance\n",
    "     # Import your model class\n",
    "    \n",
    "    # Use the same model parameters as during training\n",
    "    model = SwinUNETR(\n",
    "        img_size=(128, 128, 128),  # Assuming these were your parameters\n",
    "        in_channels=4,\n",
    "        out_channels=3,\n",
    "        feature_size=48,\n",
    "        depths=(2, 2, 2, 2),\n",
    "        num_heads=(3, 6, 12, 24),\n",
    "        norm_name=\"instance\",\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize metrics\n",
    "    val_dice = 0\n",
    "    class_dice = {'ET': 0, 'WT': 0, 'TC': 0}\n",
    "    \n",
    "    # Define the dice coefficient functions (assuming they're already defined)\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"Testing model on validation data...\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate average Dice score\n",
    "            dice_score = multiclass_dice_coefficient(outputs, targets)\n",
    "            val_dice += dice_score.item()\n",
    "            \n",
    "            # Calculate class-wise Dice scores\n",
    "            scores = class_wise_dice_coefficient(outputs, targets)\n",
    "            class_dice['ET'] += scores[0]\n",
    "            class_dice['WT'] += scores[1]\n",
    "            class_dice['TC'] += scores[2]\n",
    "    \n",
    "    # Average over validation dataset\n",
    "    val_dice /= len(val_loader)\n",
    "    for key in class_dice:\n",
    "        class_dice[key] /= len(val_loader)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Average Dice score: {val_dice:.4f}\")\n",
    "    print(f\"Class-wise Dice scores:\")\n",
    "    print(f\"  ET: {class_dice['ET']:.4f}\")\n",
    "    print(f\"  WT: {class_dice['WT']:.4f}\")\n",
    "    print(f\"  TC: {class_dice['TC']:.4f}\")\n",
    "    \n",
    "    # Return metrics\n",
    "    return {\n",
    "        'avg_dice': val_dice,\n",
    "        'class_dice': class_dice\n",
    "    }\n",
    "\n",
    "# Create a function to compare different models\n",
    "def compare_models(model_paths, val_loader, device=None):\n",
    "    \"\"\"\n",
    "    Compare multiple models on validation data\n",
    "    \n",
    "    Args:\n",
    "        model_paths: List of paths to saved model checkpoints\n",
    "        val_loader: Validation data loader\n",
    "        device: Device to run the models on\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for path in model_paths:\n",
    "        model_name = os.path.basename(path)\n",
    "        print(f\"\\nEvaluating model: {model_name}\")\n",
    "        results[model_name] = load_and_test_model(path, val_loader, device)\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"{'Model Name':<25} {'Avg Dice':<10} {'ET':<10} {'WT':<10} {'TC':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for model_name, metrics in results.items():\n",
    "        dice = metrics['avg_dice']\n",
    "        class_dice = metrics['class_dice']\n",
    "        print(f\"{model_name:<25} {dice:<10.4f} {class_dice['ET']:<10.4f} {class_dice['WT']:<10.4f} {class_dice['TC']:<10.4f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "\n",
    "# Example usage\n",
    "model_paths = [\n",
    "    '/workspace/checkpoints/swin_unetr_best_dice.pth',\n",
    "    '/workspace/checkpoints/swin_unetr_best_loss.pth',\n",
    "    '/workspace/checkpoints/swin_unetr_epoch_10.pth'\n",
    "]\n",
    "\n",
    "# First, create your validation loader using the same function you used for training\n",
    "# This assumes you have already defined the necessary functions  # Import your data loader function\n",
    "\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    data_dir=\"workspace/data\",\n",
    "    batch_size=2,\n",
    "    patch_size=(128, 128, 128),\n",
    "    train_ratio=0.8\n",
    ")\n",
    "\n",
    "# Compare the models\n",
    "compare_models(model_paths, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479f32f-8ce4-4f36-95b3-6049d2e1edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model_path, val_loader, num_samples=3, device=None):\n",
    "    \"\"\"\n",
    "    Visualize predictions from a trained model on validation data\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model checkpoint\n",
    "        val_loader: Validation data loader\n",
    "        num_samples: Number of samples to visualize\n",
    "        device: Device to run the model on\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load the model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Create a new model instance\n",
    " # Import your model class\n",
    "    \n",
    "    model = SwinUNETR(\n",
    "        img_size=(128, 128, 128),\n",
    "        in_channels=4,\n",
    "        out_channels=3,\n",
    "        feature_size=48,\n",
    "        depths=(2, 2, 2, 2),\n",
    "        num_heads=(3, 6, 12, 24),\n",
    "        norm_name=\"instance\",\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the state dictionary\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get samples from validation loader\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            samples.append((inputs, targets))\n",
    "            if len(samples) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Create visualizations\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(samples):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Apply sigmoid to convert outputs to probabilities\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # Convert to numpy for visualization\n",
    "        inputs_np = inputs.cpu().numpy()[0]  # [C, H, W, D]\n",
    "        targets_np = targets.cpu().numpy()[0]  # [3, H, W, D]\n",
    "        outputs_np = outputs.cpu().numpy()[0]  # [3, H, W, D]\n",
    "        \n",
    "        # Find a good slice to visualize (middle slice with tumor)\n",
    "        tumor_area_per_slice = np.sum(targets_np[1], axis=(0, 1))  # Use WT for finding tumor area\n",
    "        good_slice = np.argmax(tumor_area_per_slice)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        \n",
    "        # Show input modalities\n",
    "        modality_names = ['T1', 'T1ce', 'T2', 'FLAIR']\n",
    "        for j in range(4):\n",
    "            axes[0, j].imshow(inputs_np[j, :, :, good_slice], cmap='gray')\n",
    "            axes[0, j].set_title(f'Input: {modality_names[j]}')\n",
    "            axes[0, j].axis('off')\n",
    "        \n",
    "        # Show target segmentations\n",
    "        class_names = ['ET', 'WT', 'TC']\n",
    "        for j in range(3):\n",
    "            axes[1, j].imshow(targets_np[j, :, :, good_slice], cmap='hot')\n",
    "            axes[1, j].set_title(f'Target: {class_names[j]}')\n",
    "            axes[1, j].axis('off')\n",
    "        \n",
    "        # Show predicted segmentations\n",
    "        for j in range(3):\n",
    "            axes[2, j].imshow(outputs_np[j, :, :, good_slice], cmap='hot')\n",
    "            axes[2, j].set_title(f'Prediction: {class_names[j]}')\n",
    "            axes[2, j].axis('off')\n",
    "        \n",
    "        # Show overlay of all predictions on FLAIR\n",
    "        flair = inputs_np[3, :, :, good_slice]\n",
    "        flair_norm = (flair - flair.min()) / (flair.max() - flair.min())\n",
    "        \n",
    "        # Create RGB overlay\n",
    "        overlay = np.zeros((flair.shape[0], flair.shape[1], 3))\n",
    "        overlay[:, :, 0] = outputs_np[0, :, :, good_slice]  # ET in red\n",
    "        overlay[:, :, 1] = outputs_np[1, :, :, good_slice]  # WT in green\n",
    "        overlay[:, :, 2] = outputs_np[2, :, :, good_slice]  # TC in blue\n",
    "        \n",
    "        # Show overlay\n",
    "        axes[1, 3].imshow(flair_norm, cmap='gray')\n",
    "        axes[1, 3].imshow(overlay, alpha=0.5)\n",
    "        axes[1, 3].set_title('Overlay on FLAIR')\n",
    "        axes[1, 3].axis('off')\n",
    "        \n",
    "        # Empty plot\n",
    "        axes[2, 3].axis('off')\n",
    "        \n",
    "        # Set title for the whole figure\n",
    "        plt.suptitle(f'Sample {i+1} - Slice {good_slice}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'sample_{i+1}_predictions.png')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "visualize_predictions('/workspace/checkpoints/swin_unetr_best_dice.pth', val_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cba170-6f42-4813-9e80-c457ca5c3b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fffa6f1-c199-4d94-bf48-48eb0ce7524c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32641cf-9c20-4e70-a4e9-05dca31d08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Visualize training history\n",
    "    visualize_training_history('/workspace/checkpoints/training_history.json')\n",
    "    \n",
    "    # 2. Create validation loader\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        data_dir=\"workspace/data\",\n",
    "        batch_size=1,\n",
    "        patch_size=(128, 128, 128),\n",
    "        train_ratio=0.8\n",
    "    )\n",
    "    \n",
    "    # 3. Compare different model checkpoints\n",
    "    model_paths = [\n",
    "        '/workspace/checkpoints/swin_unetr_best_dice.pth',\n",
    "        '/workspace/checkpoints/swin_unetr_best_loss.pth',\n",
    "        '/workspace/checkpoints/swin_unetr_epoch_10.pth'\n",
    "    ]\n",
    "    compare_models(model_paths, val_loader)\n",
    "    \n",
    "    # 4. Visualize predictions from the best model\n",
    "    visualize_predictions('/workspace/checkpoints/swin_unetr_best_dice.pth', val_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a19ac-eca4-4d9f-a595-4ece68d33876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Plot the training history with class-wise metrics\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot loss\n",
    "    axs[0, 0].plot(history['train_loss'], label='Train Loss')\n",
    "    axs[0, 0].plot(history['val_loss'], label='Val Loss')\n",
    "    axs[0, 0].set_title('Loss')\n",
    "    axs[0, 0].set_xlabel('Epoch')\n",
    "    axs[0, 0].set_ylabel('Loss')\n",
    "    axs[0, 0].legend()\n",
    "    \n",
    "    # Plot average Dice score\n",
    "    axs[0, 1].plot(history['train_dice'], label='Train Dice')\n",
    "    axs[0, 1].plot(history['val_dice'], label='Val Dice')\n",
    "    axs[0, 1].set_title('Average Dice Score')\n",
    "    axs[0, 1].set_xlabel('Epoch')\n",
    "    axs[0, 1].set_ylabel('Dice Score')\n",
    "    axs[0, 1].legend()\n",
    "    \n",
    "    # Plot training class-wise Dice scores\n",
    "    axs[1, 0].plot(history['train_dice_et'], label='ET')\n",
    "    axs[1, 0].plot(history['train_dice_wt'], label='WT')\n",
    "    axs[1, 0].plot(history['train_dice_tc'], label='TC')\n",
    "    axs[1, 0].set_title('Training Class-wise Dice Scores')\n",
    "    axs[1, 0].set_xlabel('Epoch')\n",
    "    axs[1, 0].set_ylabel('Dice Score')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    # Plot validation class-wise Dice scores\n",
    "    axs[1, 1].plot(history['val_dice_et'], label='ET')\n",
    "    axs[1, 1].plot(history['val_dice_wt'], label='WT')\n",
    "    axs[1, 1].plot(history['val_dice_tc'], label='TC')\n",
    "    axs[1, 1].set_title('Validation Class-wise Dice Scores')\n",
    "    axs[1, 1].set_xlabel('Epoch')\n",
    "    axs[1, 1].set_ylabel('Dice Score')\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec069bd7-00c1-4926-b016-6030464aa46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32455f69-fc7e-419c-b586-5567c3557d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b1b2a-a834-4949-b8dc-e3ccd2d31f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d69d0a-f34b-4497-abc4-39e312718d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def visualize_tumor_classes(seg_path, slice_idx=None):\n",
    "    \"\"\"\n",
    "    Visualize the three tumor classes (ET, WT, TC) from a BraTS segmentation mask\n",
    "    Args:\n",
    "        seg_path: Path to the segmentation mask (.nii.gz)\n",
    "        slice_idx: Slice index to visualize (if None, will find the middle slice with tumor)\n",
    "    \"\"\"\n",
    "    # Load segmentation mask\n",
    "    seg_nii = nib.load(seg_path)\n",
    "    seg_data = seg_nii.get_fdata()\n",
    "    \n",
    "    # Create binary masks for each class\n",
    "    # In BraTS: label 1 = necrotic tumor core, label 2 = peritumoral edema, label 4 = enhancing tumor\n",
    "    mask_et = (seg_data == 4).astype(np.float32)  # Enhancing Tumor\n",
    "    mask_wt = ((seg_data == 1) | (seg_data == 2) | (seg_data == 4)).astype(np.float32)  # Whole Tumor\n",
    "    mask_tc = ((seg_data == 1) | (seg_data == 4)).astype(np.float32)  # Tumor Core\n",
    "    \n",
    "    # If slice_idx is not provided, find a good slice to display\n",
    "    if slice_idx is None:\n",
    "        # Find the slice with the largest tumor area\n",
    "        tumor_area_per_slice = np.sum(mask_wt, axis=(0, 1))\n",
    "        slice_idx = np.argmax(tumor_area_per_slice)\n",
    "    \n",
    "    # Plot the three classes\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Define colors\n",
    "    color_et = np.array([[0, 0, 0, 0], [1, 0, 0, 1]])  # Red for ET\n",
    "    color_wt = np.array([[0, 0, 0, 0], [0, 1, 0, 1]])  # Green for WT\n",
    "    color_tc = np.array([[0, 0, 0, 0], [0, 0, 1, 1]])  # Blue for TC\n",
    "    \n",
    "    cmap_et = ListedColormap(color_et)\n",
    "    cmap_wt = ListedColormap(color_wt)\n",
    "    cmap_tc = ListedColormap(color_tc)\n",
    "    \n",
    "    # Plot each class\n",
    "    axes[0].imshow(mask_et[:, :, slice_idx], cmap=cmap_et)\n",
    "    axes[0].set_title('Enhancing Tumor (ET)')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(mask_wt[:, :, slice_idx], cmap=cmap_wt)\n",
    "    axes[1].set_title('Whole Tumor (WT)')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    axes[2].imshow(mask_tc[:, :, slice_idx], cmap=cmap_tc)\n",
    "    axes[2].set_title('Tumor Core (TC)')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Tumor Classes Visualization (Slice {slice_idx})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Let's also create a combined visualization with an MRI background\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    # Create a combined RGB image\n",
    "    rgb_img = np.zeros((seg_data.shape[0], seg_data.shape[1], 3))\n",
    "    rgb_img[:, :, 0] = mask_et[:, :, slice_idx]  # Red channel - ET\n",
    "    rgb_img[:, :, 1] = mask_wt[:, :, slice_idx]  # Green channel - WT\n",
    "    rgb_img[:, :, 2] = mask_tc[:, :, slice_idx]  # Blue channel - TC\n",
    "    \n",
    "    ax.imshow(rgb_img)\n",
    "    ax.set_title('Combined Tumor Classes (Red=ET, Green=WT, Blue=TC)')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Use the correct path based on your directory structure\n",
    "visualize_tumor_classes('workspace/data/BraTS2021_00000/BraTS2021_00000_seg.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6e91a-d073-4ffb-b1ae-0a15312d40a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a more informative visualization with MRI background\n",
    "def visualize_tumor_with_background(seg_path, flair_path, slice_idx=None):\n",
    "    # Load segmentation mask\n",
    "    seg_nii = nib.load(seg_path)\n",
    "    seg_data = seg_nii.get_fdata()\n",
    "    \n",
    "    # Load FLAIR for background\n",
    "    flair_nii = nib.load(flair_path)\n",
    "    flair_data = flair_nii.get_fdata()\n",
    "    \n",
    "    # Create binary masks for each class\n",
    "    mask_et = (seg_data == 4).astype(np.float32)\n",
    "    mask_wt = ((seg_data == 1) | (seg_data == 2) | (seg_data == 4)).astype(np.float32)\n",
    "    mask_tc = ((seg_data == 1) | (seg_data == 4)).astype(np.float32)\n",
    "    \n",
    "    # If slice_idx is not provided, find a good slice to display\n",
    "    if slice_idx is None:\n",
    "        tumor_area_per_slice = np.sum(mask_wt, axis=(0, 1))\n",
    "        slice_idx = np.argmax(tumor_area_per_slice)\n",
    "    \n",
    "    # Normalize FLAIR for background\n",
    "    flair_slice = flair_data[:, :, slice_idx]\n",
    "    flair_min, flair_max = np.min(flair_slice), np.max(flair_slice)\n",
    "    if flair_max > flair_min:  # Avoid division by zero\n",
    "        flair_norm = (flair_slice - flair_min) / (flair_max - flair_min)\n",
    "    else:\n",
    "        flair_norm = flair_slice\n",
    "    \n",
    "    # Plot with MRI background\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "    \n",
    "    # Background FLAIR only\n",
    "    axes[0, 0].imshow(flair_norm, cmap='gray')\n",
    "    axes[0, 0].set_title('FLAIR MRI')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # Overlay ET on FLAIR\n",
    "    axes[0, 1].imshow(flair_norm, cmap='gray')\n",
    "    axes[0, 1].imshow(mask_et[:, :, slice_idx], cmap='hot', alpha=0.5)\n",
    "    axes[0, 1].set_title('Enhancing Tumor (ET)')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # Overlay WT on FLAIR\n",
    "    axes[1, 0].imshow(flair_norm, cmap='gray')\n",
    "    axes[1, 0].imshow(mask_wt[:, :, slice_idx], cmap='winter', alpha=0.5)\n",
    "    axes[1, 0].set_title('Whole Tumor (WT)')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Overlay TC on FLAIR\n",
    "    axes[1, 1].imshow(flair_norm, cmap='gray')\n",
    "    axes[1, 1].imshow(mask_tc[:, :, slice_idx], cmap='autumn', alpha=0.5)\n",
    "    axes[1, 1].set_title('Tumor Core (TC)')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle(f'Tumor Segmentation Visualization (Slice {slice_idx})', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with the correct paths\n",
    "visualize_tumor_with_background(\n",
    "    'workspace/data/BraTS2021_00000/BraTS2021_00000_seg.nii.gz',\n",
    "    'workspace/data/BraTS2021_00000/BraTS2021_00000_flair.nii.gz'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51964fa2-1bb1-48e0-9a30-89fcde8d99fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
